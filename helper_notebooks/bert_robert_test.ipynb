{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2253631a",
   "metadata": {},
   "source": [
    "https://skimai.com/fine-tuning-bert-for-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8064ed15",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/satyampd/imdb-sentiment-analysis-using-bert-w-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d39ec222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "from datasets import load_dataset\n",
    "import seaborn as sns\n",
    "import plotly.express as ex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241ad4e4",
   "metadata": {},
   "source": [
    "## Helper funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65d2ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_available_device():\n",
    "    if torch.cuda.is_available():       \n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "        print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d852ffeb",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727a6729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "DEVICE = check_available_device()\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8325823",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ca77f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/erthax/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Found cached dataset imdb (/home/erthax/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    }
   ],
   "source": [
    "dataset_train = load_dataset(\"imdb\", split=\"train\")\n",
    "dataset_test = load_dataset(\"imdb\", split=\"test\")\n",
    "train_data = pd.DataFrame(dataset_train)\n",
    "test_df = pd.DataFrame(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2d05277",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_data, test_size=0.1, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afcbefd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17785</th>\n",
       "      <td>If I'm going to watch a porn movie, I prefer i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14913</th>\n",
       "      <td>That magical moment in life, that point betwee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>I'm among millions who consider themselves Car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18761</th>\n",
       "      <td>I was lucky enough to catch this film finally ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20738</th>\n",
       "      <td>If you want Scream or anything like the big-st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8262</th>\n",
       "      <td>Hiya folks,&lt;br /&gt;&lt;br /&gt;Well, this movie sucks ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878</th>\n",
       "      <td>This movie was so ridiculous i never even fini...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>This is the kind of movie which shows the pauc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10114</th>\n",
       "      <td>The documentary revolves around Eva Mozes Kor,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22764</th>\n",
       "      <td>The \"Amazing Mr. Williams\" stars Melvyn Dougla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "17785  If I'm going to watch a porn movie, I prefer i...      1\n",
       "14913  That magical moment in life, that point betwee...      1\n",
       "9323   I'm among millions who consider themselves Car...      0\n",
       "18761  I was lucky enough to catch this film finally ...      1\n",
       "20738  If you want Scream or anything like the big-st...      1\n",
       "...                                                  ...    ...\n",
       "8262   Hiya folks,<br /><br />Well, this movie sucks ...      0\n",
       "2878   This movie was so ridiculous i never even fini...      0\n",
       "7597   This is the kind of movie which shows the pauc...      0\n",
       "10114  The documentary revolves around Eva Mozes Kor,...      0\n",
       "22764  The \"Amazing Mr. Williams\" stars Melvyn Dougla...      1\n",
       "\n",
       "[22500 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd46dbb9",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "498d6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in data['text']:\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded_text['input_ids'])\n",
    "        attention_masks.append(encoded_text['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(data['label'].values)\n",
    "\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce05c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, train_attention_masks, train_labels = preprocess_data(train_df, TOKENIZER)\n",
    "val_input_ids, val_attention_masks, val_labels = preprocess_data(val_df, TOKENIZER)\n",
    "test_input_ids, test_attention_masks, test_labels = preprocess_data(test_df, TOKENIZER)\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc6ad607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training steps: 2813it [18:23,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch: 0, Step: 2812, Loss: 0.12774477899074554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation steps: 100%|███████████████████████| 313/313 [00:41<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Epoch: 0, Accuracy: 0.9284, Precision: 0.9045112781954887, Recall: 0.9585657370517928, F1 Score: 0.9307543520309477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training steps: 2813it [18:25,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch: 1, Step: 2812, Loss: 0.008139469660818577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation steps: 100%|███████████████████████| 313/313 [00:41<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Epoch: 1, Accuracy: 0.9404, Precision: 0.9273570324574961, Recall: 0.9561752988047809, F1 Score: 0.9415457041977247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training steps: 2813it [18:26,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch: 2, Step: 2812, Loss: 0.0007470625569112599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation steps: 100%|███████████████████████| 313/313 [00:41<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Epoch: 2, Accuracy: 0.9364, Precision: 0.9328593996840442, Recall: 0.9410358565737051, F1 Score: 0.9369297897659659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training steps: 2813it [18:28,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch: 3, Step: 2812, Loss: 0.21905921399593353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation steps: 100%|███████████████████████| 313/313 [00:41<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Epoch: 3, Accuracy: 0.9344, Precision: 0.914828897338403, Recall: 0.9585657370517928, F1 Score: 0.9361867704280156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training steps: 2813it [18:29,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch: 4, Step: 2812, Loss: 0.00019843250629492104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation steps: 100%|███████████████████████| 313/313 [00:41<00:00,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Epoch: 4, Accuracy: 0.934, Precision: 0.9373996789727127, Recall: 0.9306772908366534, F1 Score: 0.9340263894442224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training step\"):\n",
    "        input_ids, attention_masks, labels = batch\n",
    "        input_ids = input_ids.to(DEVICE)\n",
    "        attention_masks = attention_masks.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Training - Epoch: {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch in tqdm(val_dataloader, desc=\"Validation step\"):\n",
    "            input_ids, attention_masks, labels = batch\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_masks = attention_masks.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_masks)\n",
    "            _, preds = torch.max(outputs[0], dim=1)\n",
    "\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "\n",
    "        print(f'Validation - Epoch: {epoch}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa8eea2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing step: 100%|█████████████████████████| 3125/3125 [06:57<00:00,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing - Accuracy: 0.9318, Precision: 0.9375050660614412, Recall: 0.92528, F1 Score: 0.9313524177638202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in tqdm(test_dataloader, desc=\"Testing step\"):\n",
    "        input_ids, attention_masks, labels = batch\n",
    "\n",
    "        input_ids = input_ids.to(DEVICE)\n",
    "        attention_masks = attention_masks.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_masks)\n",
    "        _, preds = torch.max(outputs[0], dim=1)\n",
    "\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_labels.extend(labels.tolist())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "\n",
    "    print(f'Testing - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c7e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f810032a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
